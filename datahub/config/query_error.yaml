# TODO: generalize before opensource
# Majority of these errors come from: https://docs.google.com/spreadsheets/d/1zE8zOIlFfO9jNqFKCBcR0Z-gKCScYprxus2rMgv2OMY/edit#gid=0
# Credits to @aoh

# Presto Errors
presto:
    malformed_presto_error:
        regex: 'Should not have nextUri if failed'
        message: >
            An error ocurred.
            However, the Presto or Hive engine returned a malformed error.
            Please click on the "Tracking URL" button to find out the original error.
            This should not happen any more after May 2018.
    need_predicate_error:
        regex: 'No partition predicate found for Alias'
        message: >
            Please add filter based on partitions of the table (e.g. where dt=... )
    presto_worker_nodes_error:
        regex: 'Encountered too many errors talking to a worker node'
        message: >
            This is probably a transient issue.
            But if the issue persists after 15 minutes, ask presto owner for help.
    extra_comma_before_from:
        regex: "extraneous input 'FROM' expecting"
        message: >
            Hi, you problably have a comma before the FROM clause, after the last column in your select clause. Please remove that comma.
    query_exceeded_local_memory:
        regex: 'Query exceeded local memory limit'
        message: >
            See if the query size can be reduced or if sampling the data is an option.
            If not, you'll need to rewrite the query for Hive.
    reading_past_rle_stream:
        regex: 'Reading past RLE/BitPacking stream.'
        message: >
            This is an issue with the current Parquet reader.
            Preface query with the following:set session hive.parquet_optimized_reader_enabled=false;
    hive_cursor_error:
        regex: 'HIVE_CURSOR_ERROR'
        message: >
            This could be due to an incompatiblity issue between the table's schema and Presto.
            You'll need to write this query in Hive.
    error_opening_split:
        regex: 'Error opening Hive split'
        message: >
            Most frequently occurs when working on tables that are not stored in the right format.
            This error can also happen if the data you want to query is no longer on s3.
    access_denied:
        regex: 'Access Denied'
        message: >
            Get PII access and run with Presto/Hive (PII) engine
    invalid_function:
        regex: '(Invalid Function)|(Function .* not registered)'
        message: >
            Typically happens when copying over queries that were previously run on Hive or vice versa.
            You'll need to convert any Hive functions to their Presto equivalent.
            <a href="https://w.pinadmin.com/pages/viewpage.action?spaceKey=DataTeam&title=Presto+User+Guide#PrestoUserGuide-MigratingFromHive:">Here is a starter doc for that</a>
    java_lang_unsupported:
        regex: 'java.lang.UnsupportedOperationException: '
        message: >
            Try prefacing the query with the following
            set session hive.parquet_optimized_reader_enabled=false;
            Alternatively, some users have found success by waiting and then trying again
    cannot_drop_table:
        regex: 'Cannot drop table'
        message: >
            Can't create or drop a table in Presto. Do it in Hive.
    too_many_splits:
        regex: 'Too many splits'
        message: >
            The input data is too large.
            Make sure you're using partitions (like the 'dt' field).
            Use sampling if that is an option.
            Write the query in Hive.
    error_fetching_results:
        regex: 'Error fetching results'
        message: >
            A transient issue that occurs from time to time, seems to be the result of bad worker nodes.
            You'll have to wait the issue fixes itself.
            Recommendation would to be try again immediately but if that fails, come back to the query at a later time.
    unexpected_status_code:
        regex: 'Unexpected status code 500'
        message: >
            A corrupted task output. Retry again or in a few minutes.
    remote_page_is_too_large:
        regex: 'Remote page is too large'
        message: >
            The table that was written likely has rows that are written with very large row outputs.
            Check if there is another table that can be used.
            If not, try writing the query in Hive.
    no_node_available:
        regex: 'No nodes available to run query'
        message: >
            Workers were temporarily unavailable.
            Retry again or in a few minutes
    hive_table_corrupt:
        regex: 'Hive table is corrupt'
        message: >
            Due to a bucket config that Presto is strict about.
            You'll need to write this query in Hive.
    table_does_not_exist:
        regex: 'Table [0-9a-zA-Z_\.]+ does not exist'
        message: >
            Verify table exists.
            If table exists, there may just not be any data for the specific partition you filtered on.
    key_not_present_in_map:
        regex: 'Key not present in map'
        message: >
            May occur if using aux_data[‘key_string’].
            This was formerly supported.
            Try using element_at instead (link to the right for more info).
hive:
    # Hive Errors
    tsocket_zerobyte_error:
        regex: 'TSocket read 0 bytes'
        message: >
            This error happened due to a HiveServer2 restart. Please check with hive owner
    standard_error:
        regex: 'Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask'
        message: >
            In order to see the "true" error, you'll need to look at the failed task logs.
    no_partition_predicate_found:
        regex: 'No partition predicate found'
        message: >
            Query will run faster if you do use partitions, like the 'dt' field.
            However, if you don't want partitions, add: SET hive.mapred.mode=nonstrict;
    very_large_file:
        regex: 'Invalid start position or invalid end position of the file'
        message: >
            Increase split size to 2GB (or even larger if necessary)
    java_heap_space:
        regex: 'Java heap space'
        message: |
            Driver ran out of memory. Please try the following
            1) bump up reducer memory
            2) set hive.groupby.skewin.data=True;
    too_many_mappers:
        regex: 'Too many mappers'
        message: |
            There are too many mappers for the job (> 15k). Please try
            1) set split size for the input data
    max_dynamic_partitions:
        regex: 'Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row'
        message: |
            Hive Query is checking too many partitions at once. Try
            1) Setup following HIVE settings
            set pinterest.script_container.mb=2048;
            set hadoop.client.opts="-Xmx1800m";
            2) Cut short the date range if you can.
    file_already_exists_exception:
        regex: 'org.apache.hadoop.fs.FileAlreadyExistsException'
        message: >
            Please refer to <a href="https://w.pinadmin.com/pages/viewpage.action?spaceKey=DataTeam&title=Hive+User+Guide#HiveUserGuide-5.16FileAlreadyExistsExceptioninyourquerytask">here</a> to resolve this issue.

sparksql:
    # Spark SQL Errors
    oom_driver_killed:
        regex: '01] is running beyond physical memory limits'
        message: >
            Driver ran out of memory, please increase driver memory (driver.memory) by 2x and retry the query. You can also refer to the Spark Guide for more details on tuning.

    oom_executor_killed:
        regex: 'is running beyond physical memory limits'
        message: >
            Executor ran out of memory, please increase the number of partitions (spark.sql.shuffle.partitions) if the error happened during reduce phase or reduce split size if error happened during initial data read using split configs

    heap_space_driver:
        regex: 'Java heap space'
        message: >
            Driver ran out of memory, please increase driver memory (driver.memory) by 2x and retry the query. You can also refer to the Spark Guide for more details on tuning.

    union_error:
        regex: 'Union :- Project'
        message: >
            Union views are currently not supported

common:
    # Common Errors
    part_number_error:
        regex: 'Part number must be an integer between 1 and 10000, inclusive'
        message: >
            Your query result exceeded DataHub's maximum limit of 100GB, consider insert result to a table instead.
