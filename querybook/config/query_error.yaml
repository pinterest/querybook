# Presto Errors
presto:
    need_predicate_error:
        regex: 'No partition predicate found for Alias'
        message: >
            Please add filter based on partitions of the table (e.g. where dt=... )
    presto_worker_nodes_error:
        regex: 'Encountered too many errors talking to a worker node'
        message: >
            This is probably a transient issue.
            But if the issue persists after 15 minutes, ask presto owner for help.
    extra_comma_before_from:
        regex: "extraneous input 'FROM' expecting"
        message: >
            You problably have a comma before the FROM clause, after the last column in your select clause. Please remove that comma.
    query_exceeded_local_memory:
        regex: 'Query exceeded local memory limit'
        message: >
            See if the query size can be reduced or if sampling the data is an option.
    reading_past_rle_stream:
        regex: 'Reading past RLE/BitPacking stream.'
        message: >
            This is an issue with the current Parquet reader.
            Preface query with the following:set session hive.parquet_optimized_reader_enabled=false;
    hive_cursor_error:
        regex: 'HIVE_CURSOR_ERROR'
        message: >
            This could be due to an incompatiblity issue between the table's schema and Presto.
    error_opening_split:
        regex: 'Error opening Hive split'
        message: >
            Most frequently occurs when working on tables that are not stored in the right format.
            This error can also happen if the data you want to query is no longer on s3.
    access_denied:
        regex: 'Access Denied'
        message: >
            Presto cannot access the underlying data.
    invalid_function:
        regex: '(Invalid Function)|(Function .* not registered)'
        message: >
            Typically happens when copying over queries that were previously run on Hive or vice versa.
            You'll need to convert any Hive functions to their Presto equivalent.
            <a href="https://prestodb.io/docs/current/migration/from-hive.html">Here is a starter doc for that</a>
    java_lang_unsupported:
        regex: 'java.lang.UnsupportedOperationException: '
        message: >
            Try prefacing the query with the following
            set session hive.parquet_optimized_reader_enabled=false;
            Alternatively, some users have found success by waiting and then trying again
    too_many_splits:
        regex: 'Too many splits'
        message: >
            The input data is too large.
            Make sure you're using partitions (like the 'dt' field).
            Use sampling if that is an option.
    error_fetching_results:
        regex: 'Error fetching results'
        message: >
            A transient issue that occurs from time to time, seems to be the result of bad worker nodes.
            You'll have to wait the issue fixes itself.
            Recommendation would to be try again immediately but if that fails, come back to the query at a later time.
    unexpected_status_code:
        regex: 'Unexpected status code 500'
        message: >
            A corrupted task output. Retry again or in a few minutes.
    remote_page_is_too_large:
        regex: 'Remote page is too large'
        message: >
            The table that was written likely has rows that are written with very large row outputs.
            Check if there is another table that can be used.
    no_node_available:
        regex: 'No nodes available to run query'
        message: >
            Workers were temporarily unavailable.
            Retry again or in a few minutes
    hive_table_corrupt:
        regex: 'Hive table is corrupt'
        message: >
            Due to a bucket config that Presto is strict about.
    table_does_not_exist:
        regex: 'Table [0-9a-zA-Z_\.]+ does not exist'
        message: >
            Verify table exists.
            If table exists, there may just not be any data for the specific partition you filtered on.
    key_not_present_in_map:
        regex: 'Key not present in map'
        message: >
            May occur if using aux_data[‘key_string’].
            This was formerly supported.
            Try using element_at instead (link to the right for more info).
hive:
    # Hive Errors
    tsocket_zerobyte_error:
        regex: 'TSocket read 0 bytes'
        message: >
            This error happened due to a HiveServer2 restart. Please check with hive owner.
    standard_error:
        regex: 'Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask'
        message: >
            In order to see the "true" error, you'll need to look at the failed task logs.
    no_partition_predicate_found:
        regex: 'No partition predicate found'
        message: >
            Query will run faster if you do use partitions, like the 'dt' field.
            However, if you don't want partitions, add: SET hive.mapred.mode=nonstrict;
    very_large_file:
        regex: 'Invalid start position or invalid end position of the file'
        message: >
            Increase split size to 2GB (or even larger if necessary)
    java_heap_space:
        regex: 'Java heap space'
        message: |
            Driver ran out of memory. Please try the following
            1) bump up reducer memory
            2) set hive.groupby.skewin.data=True;
    too_many_mappers:
        regex: 'Too many mappers'
        message: |
            There are too many mappers for the job (> 15k). Please try
            1) set split size for the input data
    file_already_exists_exception:
        regex: 'org.apache.hadoop.fs.FileAlreadyExistsException'
        message: >
            Try to add set hive.exec.sink_operator.delete_target_if_exists=true or Keep speculative execution on, turn off directfileoutputcommitter.
common:
    # Common Errors
    part_number_error:
        regex: 'Part number must be an integer between 1 and 10000, inclusive'
        message: >
            Your query result exceeded Querybook's maximum limit of 100GB, consider insert result to a table instead.
